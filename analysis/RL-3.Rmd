---
title: "Reinforcement Learning Part 3: Model-free policy evaluation"
author: "Kaiqian Zhang"
date: "6/21/2019"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dynamic programming policy evaluation

- $V^{\pi}(s) \approx \text{E}_{\pi}[r_t + \gamma V_{k-1} | s_t=s]$.

- Requires model of MDP M (i.e requires dynamic model P and reward model R).

- Bootstraps future return using value estimate $V_{k-1}$.

- Requires Markov assumption: bootstrapping regardless of history.

What if we do not have a model?

## Monte Carlo policy evaluation

- Remember $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots$ in MDP $M$ under policy $\pi$.

- $V^{\pi}(s) = \text{E}_{T\sim \pi}[G_t|s_t=s]$, where expectation over trajectories $T$ generated by following $\pi$.

- Simple idea: Value = mean return.

- If trajectories are all finite, sample set of trajectories & all average returns.

- Does not require MDP dynamics or rewards.

- Does not assume state is Markov.

### Algorithm

- Initialize $N(s) = 0$ (the number of times visited state $s$), $G(s) = 0,  \forall s \in S$.

- Loop: 

    - Sample episode $i = s_{i，1}, a_{i,1}, r_{i,1}, s_{i，2}, a_{i,2}, r_{i,2},\dots, s_{i,T_i}$.
    
    - Define $G = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots.$
    
    - For each state $s$ visited in episode $i$:
        
        - For **first or every** time $t$ that state $s$ is visited in episode $i$:
        
            - $N(s) = N(s)+1$, $G(s) = G(s) + G_{i,t}$.
            
            - Update estimate $V^{\pi}(s) = G(s)/N(s)$.
            
### Limitation

- Requires a lot of data.

- Requires episodic settings: Episode must end before data from that episode can be used to update the value function.

## MSE

- Mean squared error (MSE) of an estimator $\hat{\theta}$ is:
$$
\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Bias}_{\theta}(\hat{\theta})^2.
$$

## Temporal Difference (TD) learning

### Overview

- Combination of Monte Carlo and dynamic programming methods.

- Model-free.

- Bootstraps and samples.

- Can be used in episodic or infinite-horizon non-episodic settings.

- Immediately updates estimate of $V$ after each $(s,a,r,s')$ tuple.

### Understand TD

- Aim: estimate $V^{\pi}(s)$ given episodes generated under policy $\pi$.

    - $s_1,a_1,r_1,s_2,a_2,r_2,\dots$ where the actions are sampled from $\pi$.
  
- Recall definitions for $G_t$ and $V^{\pi}(s)$.

- In incremental every-visit MC, update estimate using 1 sample of return (for the current ith episode)

$$
V^{\pi}(s) = V^{\pi}(s) + \alpha(G_{i,t}-V^{\pi}(s)).
$$

- Insight: incorporating bootstrapping to estimate $V^{\pi}(s)$:

$$
V^{\pi}(s_t) = V^{\pi}(s_t) + \alpha([r_t+\gamma V^{\pi}(s_{t+1})]-V^{\pi}(s_t)).
$$

### TD algorithm

- Input: $\alpha$.

- Initialize $V^{\pi}(s)=0, \forall s\in S$.

- Loop

    - Sample tuple $(s_t, a_t, r_t, s_{t+1}).$
    
    - $V^{\pi}(s_t) = V^{\pi}(s_t) + \alpha(\underbrace{[r_t+\gamma V^{\pi}(s_{t+1})]}_{\text{TD target}}-V^{\pi}(s_t))$.

## Model-free policy evaluation algorithms comparison

- MC

    - Unbiased.
    
    - High variance.
    
    - Consistent (converges to true) even with function approximation.


- TD

    - Some bias.
    
    - Lower variance.
    
    - TD(0) converges to zero with tabular representation.
    
    - TD(0) does not always converge with function approximation.
    
    



















