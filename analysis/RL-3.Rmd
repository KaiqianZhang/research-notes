---
title: "Reinforcement Learning Part 3: Model-free policy evaluation"
author: "Kaiqian Zhang"
date: "6/21/2019"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dynamic programming policy evaluation

- $V^{\pi}(s) \approx \text{E}_{\pi}[r_t + \gamma V_{k-1} | s_t=s]$.

- Requires model of MDP M (i.e requires dynamic model P and reward model R).

- Bootstraps future return using value estimate $V_{k-1}$.

- Requires Markov assumption: bootstrapping regardless of history.

What if we do not have a model?

## Monte Carlo policy evaluation

- Remember $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots$ in MDP $M$ under policy $\pi$.

- $V^{\pi}(s) = \text{E}_{T\sim \pi}[G_t|s_t=s]$, where expectation over trajectories $T$ generated by following $\pi$.

- Simple idea: Value = mean return.

- If trajectories are all finite, sample set of trajectories & all average returns.

- Does not require MDP dynamics or rewards.

- Does not assume state is Markov.

### Algorithm

- Initialize $N(s) = 0$ (the number of times visited state $s$), $G(s) = 0,  \forall s \in S$.

- Loop: 

    - Sample episode $i = s_{i，1}, a_{i,1}, r_{i,1}, s_{i，2}, a_{i,2}, r_{i,2},\dots, s_{i,T_i}$.
    
    - Define $G = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots.$
    
    - For each state $s$ visited in episode $i$:
        
        - For **first or every** time $t$ that state $s$ is visited in episode $i$:
        
            - $N(s) = N(s)+1$, $G(s) = G(s) + G_{i,t}$.
            
            - Update estimate $V^{\pi}(s) = G(s)/N(s)$.

## MSE

- Mean squared error (MSE) of an estimator $\hat{\theta}$ is:
$$
\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Bias}_{\theta}(\hat{\theta})^2.
$$













