---
title: "Use Clusters and DSC"
author: "Kaiqian Zhang"
date: "2/12/2019"
output:
  workflowr::wflow_html:
    code_folding: hide
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We usually want to perform large-scale simulations on clusters. And as a research beginner, running my code on cluster is really a pain on the neck. Here I take notes on how to use UChicago's rcc clusters. The simulation tool I am using is DSC, Dynamic Statistical Comparisons, from Matthew Stephens lab. DSC provides a framework for managing computational benchmarking experiments that compare several competing methods for a task across datasets or simulation scenarios.

## Run simulations using DSC

Suppose I git push a new version of my R package `susieR` and I want to run simulations using this R package on clusters.

* step 1: log into my rcc account

```
ssh kaiqianz@midway2.rcc.uchicago.edu
```

I will need to enter my password.

* step 2: load R and git clone repo for my R package. I also need to install the local version `susieR` in the R

```
module load R/3.5.1 
```

cd to the directory I want to clone the repo,

```
git clone URL the-name-I-like-for-R-package
```

Then I can open R and devtools::install() the local R package

```
cd the-name-I-like-for-R-package

R

install.package('devtools')

devtools::install()

library(susieR)

q()
```

* step 3: before running DSC, we'd better get onto an interactive node on cluster to perform large-scale simulations. I list three kinds of commands I have tried over the summer.

```
sinteractive --partition=mstephens --time=24:00:00 --mem=8G

sinteractive --exclusive --partition=broadwl --nodes=1 --time=24:00:00 

sinteractive --partition=mstephens --time=48:00:00 --mem=64G â€”cpus-per-task=28
```

* step 4: go to the DSC repo, which is a repo I created for simulation code. In this case, we have `dsc-susie`. We can first run a truncate version to see if this works before large-scale simulations.

```
cd dsc-susie

dsc benchmark.dsc target susie_v --truncate
```
After we pass the truncate version, we can run everything.

```
dsc benchmark.dsc target susie_v --replicate 50 -c 16
```

## Retrieve results from DSC and clusters or debug

Here we would use online python notebook to connect my cluster and easily open my simulation results. Note that I have `jupyterlab-server` installed on my midway. 

```
cd jupyterlab-server

sbatch jupyter-lab.sbatch 
```

I would see `Submitted batch job 57433075`.

want until showq has the job, check the following

```
showq
ll
```

```
cat nb-log-57433075.out
```

I would get a command start with `ssh` and a website looks like `localhost:9148`. Now open a new terminal and paste that `ssh` command on my own computer, not on midway. 

```
ssh -N -L 9148:10.50.221.3:9148 kaiqianz@midway2.rcc.uchicago.edu -v
```

Then we can open the website `localhost:9148` on google chrome. To retrieve the token asked from the website, we do

```
cat nb-log-57433075.err
```
to get the token.

Now we are ready to go. We have all simulation results in python notebook so that we can do analysis!

## How to debug then...
Now we are on the online jupyter notebook which contains all the error data from midway. When we ran dsc, we got the error message like `Failed to execute Rscript .sos/sim_gauss_0_7dc66a52.R`. Now go to the terminal and cat the Rscript and paste it into the jupyter notebook for your debugging!

In the terminal
```
cat .sos/sim_gauss_0_7dc66a52.R
```
And then paste everything shown on the screen to your jupyter notebook.

## Use yml to run cluster simulations

Here is a template for yml file `kaiqian_midway.yml`. If each instance in your job takes a very short time, you can submit more instances per job to save more time. 

```
DSC:
  midway2:
    description: UChicago RCC cluster Midway 2
    address: kaiqianz@midway2.rcc.uchicago.edu
    paths:
      home: /home/kaiqianz
    queue_type: pbs
    status_check_interval: 60
    max_running_jobs: 30
    max_cores: 40
    max_walltime: "36:00:00"
    max_mem: 64G
    job_template: |
      #!/bin/bash
      #SBATCH --time={walltime}
      #{partition}
      #{account}
      #SBATCH --nodes=1
      #SBATCH --ntasks-per-node={cores}
      #SBATCH --mem-per-cpu={mem//10**9}G
      #SBATCH --job-name={job_name}
      #SBATCH --output={cur_dir}/{job_name}.out
      #SBATCH --error={cur_dir}/{job_name}.err
      cd {cur_dir}
      module load R
    partition: "SBATCH --partition=broadwl"
    account: ""
    submit_cmd: sbatch {job_file}
    submit_cmd_output: "Submitted batch job {job_id}"
    status_cmd: squeue --job {job_id}
    kill_cmd: scancel {job_id}
  midway2_head:
    based_on: midway2
    address: localhost
  stephenslab:
    based_on: midway2
    max_cores: 28
    max_mem: 128G
    max_walltime: "10d"
    partition: "SBATCH --partition=mstephens"
    account: "SBATCH --account=pi-mstephens"

default:
  queue: midway2_head
  time_per_instance: 10m
  instances_per_job: 10
  n_cpu: 1
  mem_per_cpu: 2G

simulate:
  time_per_instance: 2m
  
score:
  time_per_instance: 2m
  
```

After adding this yml file to your repo, you can run the following command on an sinteractive node.

```
dsc benchmark.dsc --host kaiqian_midway.yml --replicate 50 -c 40

```







